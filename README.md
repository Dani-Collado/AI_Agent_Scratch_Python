# ğŸ¤– AI Agent con Modelos Locales

Un agente de investigaciÃ³n inteligente que utiliza **modelos de lenguaje locales** (Ollama) para realizar bÃºsquedas y responder preguntas de manera autÃ³noma.

## ğŸ“‹ DescripciÃ³n

Este proyecto implementa un agente de IA que puede buscar informaciÃ³n en la web y Wikipedia utilizando LangChain y modelos locales ejecutados con Ollama. El agente decide automÃ¡ticamente quÃ© herramientas usar segÃºn la consulta del usuario.

## âœ¨ CaracterÃ­sticas

- ğŸ  **100% Local**: Utiliza modelos de lenguaje ejecutados localmente con Ollama
- ğŸ” **BÃºsqueda Web**: IntegraciÃ³n con DuckDuckGo para bÃºsquedas en internet
- ğŸ“š **Wikipedia**: Acceso a informaciÃ³n detallada de Wikipedia
- ğŸ¤– **Agente AutÃ³nomo**: Decide quÃ© herramientas usar segÃºn el contexto
- ğŸ”’ **Privacidad**: Tus datos no salen de tu mÃ¡quina

## ğŸ› ï¸ Funciones Principales

### `main.py`

#### `check_ollama()`
Verifica que el servidor de Ollama estÃ© ejecutÃ¡ndose antes de iniciar el agente. Realiza una peticiÃ³n HTTP a `localhost:11434` para confirmar la disponibilidad del servicio.

#### `main()`
FunciÃ³n principal que:
1. Verifica la conexiÃ³n con Ollama
2. Inicializa el modelo local (Mistral)
3. Crea el agente con las herramientas disponibles
4. Procesa la consulta del usuario
5. Retorna la respuesta generada

### `tools.py`

#### `search_web(query: str)`
Herramienta que permite al agente buscar informaciÃ³n en internet usando DuckDuckGo. Retorna resultados relevantes basados en la consulta.

#### `search_wikipedia(query: str)`
Herramienta que busca informaciÃ³n detallada en Wikipedia. Ideal para obtener datos enciclopÃ©dicos y contexto histÃ³rico.

## ğŸ“¦ InstalaciÃ³n

### 1. Instalar Ollama

#### Windows
1. Descarga el instalador desde [ollama.com/download](https://ollama.com/download)
2. Ejecuta el instalador `.exe`
3. Ollama se iniciarÃ¡ automÃ¡ticamente como servicio

#### macOS
```bash
curl -fsSL https://ollama.com/install.sh | sh
```

#### Linux
```bash
curl -fsSL https://ollama.com/install.sh | sh
```

### 2. Descargar el Modelo

Una vez instalado Ollama, descarga el modelo Mistral:

```bash
ollama pull mistral
```

Otros modelos disponibles:
- `ollama pull llama2` - Llama 2 (7B)
- `ollama pull codellama` - Code Llama (especializado en cÃ³digo)
- `ollama pull phi` - Phi-2 (mÃ¡s ligero)

### 3. Verificar que Ollama estÃ¡ corriendo

```bash
ollama serve
```

O verifica con:
```bash
curl http://localhost:11434/api/tags
```

### 4. Instalar uv (Gestor de Paquetes)

Este proyecto utiliza **uv**, un gestor de paquetes de Python ultrarrÃ¡pido.

#### Windows (PowerShell)
```powershell
powershell -c "irm https://astral.sh/uv/install.ps1 | iex"
```

#### macOS/Linux
```bash
curl -LsSf https://astral.sh/uv/install.sh | sh
```

MÃ¡s informaciÃ³n en: [docs.astral.sh/uv](https://docs.astral.sh/uv/)

### 5. Instalar Dependencias del Proyecto

Con uv instalado, sincroniza las dependencias del proyecto:

```bash
uv sync
```

Esto instalarÃ¡ automÃ¡ticamente todas las dependencias definidas en `pyproject.toml`:
- langchain
- langchain-ollama
- langchain-community
- pydantic
- wikipedia
- gpt4all
- python-dotenv

## ğŸš€ Uso

1. AsegÃºrate de que Ollama estÃ© corriendo:
```bash
ollama serve
```

2. Ejecuta el agente:
```bash
python main.py
```

3. Ingresa tu consulta cuando se te solicite:
```
Â¿En que se le puede ayudar? Â¿CuÃ¡l es la capital de Francia?
```

## ğŸ“ Ejemplo de Uso

```
âœ… Ollama is running
Â¿En que se le puede ayudar? Â¿QuiÃ©n fue Albert Einstein?
Running agent...
Respuesta Limpia: Albert Einstein fue un fÃ­sico teÃ³rico alemÃ¡n...
```

## ğŸ”§ ConfiguraciÃ³n

Puedes cambiar el modelo en `main.py`:

```python
llm = ChatOllama(
    model="mistral",  # Cambia a "llama2", "phi", etc.
    timeout=120  
)
```

## ğŸ“š TecnologÃ­as

- **Python**: Lenguaje de programaciÃ³n utilizado para desarrollar el proyecto
- **UV**: Gestor e instalador de paquetes de python
- **LangChain**: Framework para aplicaciones con LLMs
- **Ollama**: EjecuciÃ³n local de modelos de lenguaje
- **DuckDuckGo**: Motor de bÃºsqueda web
- **Wikipedia API**: Acceso a contenido enciclopÃ©dico

## ğŸ¤ Contribuciones

Las contribuciones son bienvenidas. Por favor, abre un issue o pull request.

## ğŸ“„ Licencia

Este proyecto es de cÃ³digo abierto y estÃ¡ disponible bajo la licencia MIT.

---

**Nota**: Este proyecto requiere que Ollama estÃ© instalado y corriendo localmente. No se conecta a APIs externas de modelos de lenguaje.
